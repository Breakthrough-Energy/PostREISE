{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import gzip\n",
    "import os\n",
    "\n",
    "from powersimdata.scenario.scenario import Scenario\n",
    "from postreise.analyze.transmission.utilization import generate_cong_stats, get_utilization\n",
    "\n",
    "from data_processing.azure_blob_uploaders import BlobUtil\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loadzone2state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: this will cause issues if two loadzones have the same name\n",
    "loadzone2state = {\n",
    "    \"Bay Area\": \"California\",\n",
    "    \"Central California\": \"California\",\n",
    "    \"Northern California\": \"California\",\n",
    "    \"Southeast California\": \"California\",\n",
    "    \"Southwest California\": \"California\",\n",
    "\n",
    "    \"Florida Panhandle\": \"Florida\",\n",
    "    \"Florida North\": \"Florida\",\n",
    "    \"Florida South\": \"Florida\",\n",
    "\n",
    "    \"Georgia North\": \"Georgia\",\n",
    "    \"Georgia South\": \"Georgia\",\n",
    "\n",
    "    \"Chicago North Illinois\": \"Illinois\",\n",
    "    \"Illinois Downstate\": \"Illinois\",\n",
    "\n",
    "    \"Michigan Northern\": \"Michigan\",\n",
    "    \"Michigan Southern\": \"Michigan\",\n",
    "\n",
    "    \"Minnesota Northern\": \"Minnesota\",\n",
    "    \"Minnesota Southern\": \"Minnesota\",\n",
    "\n",
    "    \"Missouri East\": \"Missouri\",\n",
    "    \"Missouri West\": \"Missouri\",\n",
    "\n",
    "    \"Montana Eastern\": \"Montana\",\n",
    "    \"Montana Western\": \"Montana\",\n",
    "\n",
    "    \"New York City\": \"New York\",\n",
    "    \"Upstate New York\": \"New York\",\n",
    "\n",
    "    \"Western North Carolina\": \"North Carolina\",\n",
    "\n",
    "    \"New Mexico Eastern\": \"New Mexico\",\n",
    "    \"New Mexico Western\": \"New Mexico\",\n",
    "\n",
    "    \"Ohio River\": \"Ohio\",\n",
    "    \"Ohio Lake Erie\": \"Ohio\",\n",
    "\n",
    "    \"Pennsylvania Eastern\": \"Pennsylvania\",\n",
    "    \"Pennsylvania Western\": \"Pennsylvania\",\n",
    "\n",
    "    \"Coast\": \"Texas\",\n",
    "    \"East\": \"Texas\",\n",
    "    \"East Texas\": \"Texas\",\n",
    "    \"El Paso\": \"Texas\",\n",
    "    \"Far West\": \"Texas\",\n",
    "    \"North\": \"Texas\",\n",
    "    \"North Central\": \"Texas\",\n",
    "    \"South\": \"Texas\",\n",
    "    \"South Central\": \"Texas\",\n",
    "    \"Texas Panhandle\": \"Texas\",\n",
    "    \"West\": \"Texas\",\n",
    "\n",
    "    \"Virginia Mountains\": \"Virginia\",\n",
    "    \"Virginia Tidewater\": \"Virginia\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save dataframes to a json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_dfs_to_json_file(dataframes, path):\n",
    "    df_records = []\n",
    "    \n",
    "    print(\"Getting dataframe records...\")\n",
    "    for df in dataframes:\n",
    "        # Create a list of df rows\n",
    "        df_records += df.to_dict('records')\n",
    "        print(\".\")\n",
    "    \n",
    "    print(\"Done getting records\")\n",
    "    print(\"Saving to file:\", path)\n",
    "    with open(path, 'w') as fp:\n",
    "        json.dump(df_records, fp)\n",
    "\n",
    "    print(f\"Done! file saved at {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Scenario Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to upload all rollups for the data because\n",
    "....but not in python (╯°□°）╯︵ ┻━┻\n",
    "\n",
    "At some point we may want more rollups like week and month\n",
    "but for now we do not need them\n",
    "\n",
    "All the rollups we will create\n",
    "We do not store plant x day in cosmos\n",
    "```python\n",
    "{\n",
    "    'by_plant': {\n",
    "        'by_day': {\n",
    "            'with_curtailment': None,\n",
    "            'non_renewable': None\n",
    "        },\n",
    "        'by_year': {\n",
    "            'with_curtailment': None,\n",
    "            'non_renewable': None\n",
    "        }\n",
    "    },\n",
    "    'by_zone': {\n",
    "        'by_day': {\n",
    "            'with_curtailment': None,\n",
    "            'non_renewable': None\n",
    "        },\n",
    "        'by_year': {\n",
    "            'with_curtailment': None,\n",
    "            'non_renewable': None\n",
    "        }\n",
    "    },\n",
    "    'by_interconnect': {\n",
    "        'by_day': {\n",
    "            'with_curtailment': None,\n",
    "            'non_renewable': None\n",
    "        },\n",
    "        'by_year': {\n",
    "            'with_curtailment': None,\n",
    "            'non_renewable': None\n",
    "        }\n",
    "    },\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# PLANT x DAY\n",
    "# This makes the rest of the rollups easy!\n",
    "\n",
    "def get_plant_by_day(scenario_id, grid, pg):\n",
    "\n",
    "    print(\"\\ngetting PLANT x DAY \\n\")\n",
    "    # Roll up data by day\n",
    "    pg = pg.resample('d').sum()\n",
    "\n",
    "    # Turn timestamps into strings\n",
    "    pg.index = pg.index.strftime('%Y-%m-%d')\n",
    "\n",
    "    # Unpivot data so cols are now timestamp, plant id, generation value\n",
    "    # There are now ~900k rows: 2.5k plants * 366 days \n",
    "    pg = pg.reset_index().melt(id_vars='UTC')\n",
    "\n",
    "    pg.columns = ['timestamp', 'plant_id', 'generation']\n",
    "\n",
    "    # Add extra data\n",
    "    pg['scenario_id'] = scenario_id\n",
    "    pg['LOC_ROLLUP'] = 'PLANT'\n",
    "    pg['TIME_ROLLUP'] = 'DAY'\n",
    "    pg = pg.join(grid.plant[['zone_name', 'type', 'interconnect']], on='plant_id')\n",
    "    pg = pg.rename(columns={ 'type': 'resource_type', 'zone_name': 'zone' })\n",
    "    \n",
    "    # Replace loadzone with state name\n",
    "    pg['zone'] = pg['zone'].replace(loadzone2state)\n",
    "\n",
    "    # order cols nicely\n",
    "    pg = pg[['plant_id', 'timestamp', 'resource_type', 'zone', 'interconnect', 'scenario_id', 'LOC_ROLLUP', 'TIME_ROLLUP', 'generation']]\n",
    "\n",
    "    # TODO: make this cleaner...\n",
    "    pg_renewable = pg.loc[(pg['resource_type'] == 'solar') | (pg['resource_type'] == 'wind') | (pg['resource_type'] == 'offshore_wind')]\n",
    "    pg_nonrenewable = pg.loc[(pg['resource_type'] != 'solar') & (pg['resource_type'] != 'wind') & (pg['resource_type'] != 'offshore_wind')]\n",
    "\n",
    "    # Separate renewables and non-renewables so we can add curtailment to wind and solar\n",
    "    #     print(\"RENEWABLE\")\n",
    "    #     print(pg_renewable)\n",
    "    #     print()\n",
    "    #     print(\"NON RENEWABLE\")\n",
    "    #     print(pg_nonrenewable)\n",
    "\n",
    "    return [pg_renewable, pg_nonrenewable]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curtailment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_curtailment_by_day(scenario_id, grid, pg, pg_renewables, wind, solar):\n",
    "    print(\"\\ngetting CURTAILMENT x DAY \\n\")\n",
    "    \n",
    "    pg_by_day = pg.resample('d').sum()\n",
    "    wind_by_day = wind.resample('d').sum()\n",
    "    solar_by_day = solar.resample('d').sum()\n",
    "    \n",
    "    wind_plants = grid.plant.loc[(grid.plant['type'] == 'wind')].index\n",
    "    offshore_wind_plants = grid.plant.loc[(grid.plant['type'] == 'wind_offshore')].index\n",
    "    solar_plants = grid.plant.loc[(grid.plant['type'] == 'solar')].index\n",
    "    \n",
    "    \n",
    "    curtailed_wind = wind_by_day[wind_plants] - pg_by_day[wind_plants]\n",
    "    curtailed_offshore_wind = wind_by_day[offshore_wind_plants] - pg_by_day[offshore_wind_plants]\n",
    "    curtailed_solar = solar_by_day[solar_plants] - pg_by_day[solar_plants]\n",
    "\n",
    "    # Merge\n",
    "    curtailed = pd.merge(curtailed_wind, curtailed_solar, left_index=True, right_index=True)\n",
    "    curtailed = pd.merge(curtailed, curtailed_offshore_wind, left_index=True, right_index=True)\n",
    "\n",
    "    # Turn timestamps into strings\n",
    "    curtailed.index = curtailed.index.strftime('%Y-%m-%d')\n",
    "\n",
    "    # Unpivot data so cols are now timestamp, plant id, value\n",
    "    curtailed = curtailed.reset_index().melt(id_vars='UTC')\n",
    "\n",
    "    curtailed.columns = ['timestamp', 'plant_id', 'curtailment']\n",
    "\n",
    "    # join curtailed and pg_renewables\n",
    "    print(\"curtailed df shape\", curtailed.shape, \"pg renewables shape\", pg_renewables.shape)\n",
    "    pg_and_curtailment = pd.merge(pg_renewables, curtailed,  how='left', on=['timestamp', 'plant_id'])\n",
    "\n",
    "    #     print('Num rows where curatilment > 0:', len(pg_and_curtailment.loc[(pg_and_curtailment['curtailment'] > 0)].index))\n",
    "    #     print()\n",
    "    #     print(pg_and_curtailment)\n",
    "    return pg_and_curtailment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plant rollups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All rollups at once\n",
    "def create_list_of_pg_rollups(pg_and_curtailment, pg_nonrenewable):\n",
    "    pg_dfs = []\n",
    "    for loc_rollup in ['PLANT', 'ZONE', 'INTERCONNECT', 'USA']:\n",
    "        for time_rollup in ['DAY', 'YEAR']:\n",
    "            # Skip plant x day, we already have it\n",
    "            if loc_rollup == 'PLANT' and time_rollup == 'DAY':\n",
    "                continue\n",
    "                \n",
    "            print(f\"getting {loc_rollup} x {time_rollup}\")\n",
    "            \n",
    "\n",
    "            for pg in [pg_and_curtailment.copy(), pg_nonrenewable.copy()]:\n",
    "                pg['LOC_ROLLUP'] = loc_rollup\n",
    "                pg['TIME_ROLLUP'] = time_rollup\n",
    "\n",
    "                groupby_cols = [\n",
    "                    'timestamp', \n",
    "                    'plant_id', 'zone', 'interconnect', 'scenario_id', \n",
    "                    'LOC_ROLLUP', 'TIME_ROLLUP', \n",
    "                    'resource_type'\n",
    "                ]\n",
    "\n",
    "                if time_rollup == 'YEAR':\n",
    "                    pg['timestamp'] = '2016'\n",
    "                    \n",
    "                if loc_rollup == 'ZONE':\n",
    "                    pg = pg.drop(columns=['plant_id'])\n",
    "                    groupby_cols.remove('plant_id')\n",
    "                    \n",
    "                elif loc_rollup == 'INTERCONNECT':\n",
    "                    pg = pg.drop(columns=['plant_id', 'zone'])\n",
    "                    groupby_cols.remove('plant_id')\n",
    "                    groupby_cols.remove('zone')\n",
    "                    \n",
    "                elif loc_rollup == 'USA':\n",
    "                    pg = pg.drop(columns=['plant_id', 'zone', 'interconnect'])\n",
    "                    groupby_cols.remove('plant_id')\n",
    "                    groupby_cols.remove('zone')\n",
    "                    groupby_cols.remove('interconnect')\n",
    "\n",
    "                pg = pg.groupby(groupby_cols).sum()\n",
    "                pg = pg.reset_index()\n",
    "                \n",
    "                # Round generation and curtailment values\n",
    "                pg['generation'] = pg['generation'].apply(lambda x: round(x, 2))\n",
    "                if 'curtailment' in pg.columns:\n",
    "                    pg['curtailment'] = pg['curtailment'].apply(lambda x: round(x, 2))\n",
    "\n",
    "                pg_dfs.append(pg)\n",
    "                # print(pg)\n",
    "                # print()\n",
    "\n",
    "    print()\n",
    "    print(\"Done!\")\n",
    "    return pg_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Branch does not need rollups like Plant because for now we're only looking at data for the whole year\n",
    "\n",
    "def process_branch_data(scenario_id, scenario, grid, pf):\n",
    "    print(f\"start pf processing for {scenario_id}\")\n",
    "    branch = grid.branch.loc[(grid.branch['rateA'] != 0) & (grid.branch['branch_device_type'] == 'Line')]\n",
    "\n",
    "    print(\"getting utilization data\")\n",
    "    \n",
    "    # UTILIZATION DATA\n",
    "    util_median = get_utilization(branch, pf, median=True).T\n",
    "    util_median = util_median.rename(columns={0: 'median_utilization'})\n",
    "    \n",
    "    print(\"getting congestion stats\")\n",
    "    \n",
    "    # RISK DATA (congestion)\n",
    "    congestion_stats = generate_cong_stats(pf, grid.branch)[['risk', 'bind']]\n",
    "\n",
    "    # Combine\n",
    "    branch_with_util_and_cong = pd.concat([util_median, congestion_stats], axis=1)\n",
    "    #print(\"Median util rows that are NAN:\", branch_with_util_and_cong.loc[branch_with_util_and_cong['median_utilization'] == np.nan].size)\n",
    "    #print(\"Median util rows that are NOT NAN:\", branch_with_util_and_cong.loc[branch_with_util_and_cong['median_utilization'] != np.nan].size)\n",
    "    \n",
    "    # Replace any NANs with 0 for binding and risk\n",
    "    # TODO: check with team\n",
    "    branch_with_util_and_cong = branch_with_util_and_cong.fillna(0)\n",
    "    \n",
    "    # Set index as col so we keep the branch id when writing records to dict\n",
    "    branch_with_util_and_cong['branch_id'] = branch_with_util_and_cong.index\n",
    "    branch_with_util_and_cong = branch[['from_zone_name', 'to_zone_name', 'interconnect']].join(branch_with_util_and_cong, on='branch_id')\n",
    "    branch_with_util_and_cong = branch_with_util_and_cong.rename(columns={ 'from_zone_name': 'from_zone', 'to_zone_name': 'to_zone' })\n",
    "    \n",
    "    # Replace loadzone with state name\n",
    "    branch_with_util_and_cong[['from_zone', 'to_zone']] = branch_with_util_and_cong[['from_zone', 'to_zone']].replace(loadzone2state)\n",
    "    \n",
    "    branch_with_util_and_cong['scenario_id'] = scenario_id\n",
    "    branch_with_util_and_cong['LOC_ROLLUP'] = 'BRANCH'\n",
    "    branch_with_util_and_cong['TIME_ROLLUP'] = 'YEAR'\n",
    "    #print()\n",
    "    #print(\"Unique cong risk rows:\", branch_with_util_and_cong['risk'].unique().size)\n",
    "    #print(\"Cong bind rows greater than 0\", branch_with_util_and_cong.loc[branch_with_util_and_cong['bind'] != 0].size)\n",
    "    #print()\n",
    "    #print(branch_with_util_and_cong)\n",
    "    return branch_with_util_and_cong\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process pg and pf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pg_and_pfdata(scenario_id, path):\n",
    "    \n",
    "    print(f\"starting scenario {scenario_id}\\n\")\n",
    "    s = Scenario(str(scenario_id))\n",
    "    grid = s.state.get_grid()\n",
    "\n",
    "    print(\"\\nstarting pg processing\\n\")\n",
    "    pg = s.state.get_pg()\n",
    "    wind = s.state.get_wind()\n",
    "    solar = s.state.get_solar()\n",
    "\n",
    "    [pg_renewable, pg_nonrenewable] = get_plant_by_day(scenario_id, grid, pg)\n",
    "    pg_and_curtailment_by_day = get_curtailment_by_day(scenario_id, grid, pg, pg_renewable, wind, solar)\n",
    "\n",
    "    pg_rollups = create_list_of_pg_rollups(pg_and_curtailment_by_day, pg_nonrenewable)\n",
    "\n",
    "    print(\"\\npg processing finished, saving to file\\n\")\n",
    "\n",
    "    save_dfs_to_json_file(pg_rollups, path + f'pg_data_{scenario_id}.json')\n",
    "    print(f\"pg file save complete for {scenario_id}\\n\")\n",
    "\n",
    "\n",
    "#     pf = s.state.get_pf()\n",
    "#     processed_pf = process_branch_data(scenario_id, s, grid, pf)\n",
    "#     print(f\"\\nfinished processing pf for scenario {scenario_id}\\n\")\n",
    "#     print(\"saving file\")\n",
    "#     save_dfs_to_json_file([processed_pf], path + f'pf_data_{scenario_id}.json')\n",
    "#     print(f\"pf file save complete for scenario {scenario_id}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "path = ''\n",
    "\n",
    "scenario_ids = [544, 556, 573, 585, 594, 612, 823, 824, 1097, 1098, 1099, 1149, 1151, 1152, 1176, 1177, 1204, \n",
    "               1205, 1206, 1242, 1244, 1245, 1257, 1258, 1270, 1705, 1724, 1723]\n",
    "\n",
    "    \n",
    "for scenario_id in scenario_ids:\n",
    "    process_pg_and_pfdata(scenario_id, path)\n",
    "    \n",
    "print(\"\\nfinished!!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Plant Data\n",
    "```python\n",
    "grid_plant = [\n",
    "    {\n",
    "        capacity: 123,\n",
    "        resource_type: 'wind',\n",
    "        coords: [-45.4, 90.1],                  # [lon, lat]\n",
    "        zone: 'Washington',\n",
    "        interconnect: 'Western',\n",
    "        generation: 512034,                 # Generation for year in MW\n",
    "        curtailment: 152                    # Curtailment for year in MW\n",
    "    },\n",
    "    ...\n",
    "]\n",
    "```\n",
    "\n",
    "# Grid Branch Data\n",
    "```python\n",
    "grid_branch = [\n",
    "    {\n",
    "        capacity: 123,\n",
    "        coords: [[-45, 90], [-46, 91]],     # [[from_lon, from_lat], [to_lon, to_lat]]\n",
    "        zone: ['Washington', 'Washington'], # [from_zone, to_zone]\n",
    "        interconnect: 'Western',\n",
    "    },\n",
    "    ...\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New grid functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fix SettingWithCopyWarning\n",
    "\n",
    "# Combines columns into a new one. Drops old columns.\n",
    "def combine_cols(df, cols, new_col_name):\n",
    "    df_copy = df.copy()\n",
    "    df_copy[new_col_name] = df_copy[cols].values.tolist()\n",
    "    return df_copy.drop(columns=cols)\n",
    "\n",
    "def get_pref_suf(string, prefix, suffix):\n",
    "    return f\"{prefix}{string}{suffix}\"\n",
    "\n",
    "# Takes a bus_id col and replaces it with a column with [lon, lat]\n",
    "# also adds zone and interconnect \n",
    "# optionally, can round coordinates and then combine items with the same coordinates\n",
    "def bus2coords(\n",
    "    df, \n",
    "    grid, \n",
    "    bus_col_name='bus_id', \n",
    "    col_prefix=\"\", \n",
    "    col_suffix=\"\", \n",
    "    drop_bus_col=True, # Should be true if we are coordinate rounding\n",
    "    coordinate_rounding=0, \n",
    "    groupby_cols=[], # for coord rounding\n",
    "    agg_method={}    # for coord rounding\n",
    "):\n",
    "    bus = grid.bus[[\"lon\", \"lat\", \"zone_id\", \"interconnect\"]]\n",
    "    bus = bus.rename(columns={ 'zone_id': 'zone' })\n",
    "    \n",
    "    # Get zone info\n",
    "    bus['zone'] = bus['zone'].replace(grid.id2zone)\n",
    "    bus['zone'] = bus['zone'].replace(loadzone2state)\n",
    "    \n",
    "    # We need to update column names before joining tables\n",
    "    bus = bus.add_prefix(col_prefix).add_suffix(col_suffix)\n",
    "        \n",
    "    # Get new column names for later\n",
    "    # TODO: is there a more elegant way to do this?\n",
    "    lat_col = get_pref_suf(\"lat\", col_prefix, col_suffix)\n",
    "    lon_col = get_pref_suf(\"lon\", col_prefix, col_suffix)\n",
    "    interconnect_col = get_pref_suf(\"interconnect\", col_prefix, col_suffix)\n",
    "    zone_col = get_pref_suf(\"zone\", col_prefix, col_suffix)\n",
    "    coords_col = get_pref_suf(\"coords\", col_prefix, col_suffix)\n",
    "    \n",
    "    # Add location info to original df\n",
    "    new_df = df.join(bus, bus_col_name)\n",
    "    if (drop_bus_col):\n",
    "        new_df = new_df.drop(columns=bus_col_name)\n",
    "    \n",
    "    # If we are coordinate_rounding, combine rows by location and groupby_cols \n",
    "    # Aggregate other cols by agg_method\n",
    "    if (coordinate_rounding):\n",
    "        new_df[lat_col] = new_df[lat_col].round(coordinate_rounding)\n",
    "        new_df[lon_col] = new_df[lon_col].round(coordinate_rounding)\n",
    "\n",
    "        new_df = new_df.groupby([lat_col, lon_col, interconnect_col, zone_col] + groupby_cols).agg(agg_method)\n",
    "        new_df = new_df.reset_index()\n",
    "    \n",
    "    # combine lat and lon into coords col\n",
    "    new_df = combine_cols(new_df, [lon_col, lat_col], coords_col)\n",
    "    return new_df\n",
    "\n",
    "def get_grid_plant(grid, pg, wind, solar):\n",
    "    # get plant df\n",
    "    plant = grid.plant[['Pmax', 'type', 'bus_id']]\n",
    "    plant = plant.rename(columns={ \n",
    "        'type': 'resource_type', \n",
    "        'Pmax': 'capacity'\n",
    "    })\n",
    "\n",
    "    # get generation and curtailment\n",
    "    pg_year = pg.resample('y').sum().T\n",
    "    wind_year = wind.resample('y').sum().T\n",
    "    solar_year = solar.resample('y').sum().T\n",
    "\n",
    "    pg_year.columns = ['generation']\n",
    "    wind_year.columns = ['available']\n",
    "    solar_year.columns = ['available']\n",
    "\n",
    "    # curtailment\n",
    "    wind_and_solar = wind_year.append(solar_year)\n",
    "    wind_and_solar = wind_and_solar.join(pg_year)\n",
    "    wind_and_solar['curtailment'] = wind_and_solar['available'] - wind_and_solar['generation']\n",
    "\n",
    "    # combine grid plant, pg, and curtailment\n",
    "    pg_and_curtailment = pg_year.join(wind_and_solar[['curtailment']])\n",
    "    plant = plant.join(pg_and_curtailment)\n",
    "    plant = plant.fillna(value=0)\n",
    "    plant = plant.reset_index()\n",
    "    \n",
    "    # combine plants by location. Sum capacity, generation, and curtailment\n",
    "    plant = bus2coords(\n",
    "        plant, \n",
    "        grid, \n",
    "        coordinate_rounding=1, \n",
    "        groupby_cols=[\"resource_type\"], \n",
    "        agg_method={\"generation\": \"sum\", \"capacity\": \"sum\", \"curtailment\": \"sum\"}\n",
    "    )\n",
    "    \n",
    "    print(plant)\n",
    "    return plant\n",
    "\n",
    "# We don't combine storage and plant because the ids overlap\n",
    "# Also we're not graphing yearly generation for storage\n",
    "# Storage constantly charges and discharges, so generation for \n",
    "# the year should be close to zero. There are some inefficiencies, \n",
    "# so technically generation is negative\n",
    "def get_grid_storage(grid):\n",
    "    storage = grid.storage['gen'][['bus_id', 'Pmax']]\n",
    "    storage = storage.rename(columns={ \n",
    "        'Pmax': 'capacity' \n",
    "    })\n",
    "    storage = bus2coords(storage, grid)\n",
    "    return storage\n",
    "\n",
    "def get_grid_branch(grid, pf):\n",
    "    branch = grid.branch.loc[(grid.branch['rateA'] != 0) \n",
    "                             & (grid.branch['branch_device_type'] == 'Line')]\n",
    "\n",
    "    branch = branch[['rateA', 'from_bus_id', 'to_bus_id', 'interconnect']]\n",
    "    branch = branch.rename(columns={ 'rateA': 'capacity' })\n",
    "\n",
    "    branch = bus2coords(branch, grid, bus_col_name='from_bus_id', col_prefix=\"from_\")\n",
    "    branch = bus2coords(branch, grid, bus_col_name='to_bus_id', col_prefix=\"to_\")\n",
    "    branch = combine_cols(branch, ['from_coords', 'to_coords'], 'coords')\n",
    "    branch = combine_cols(branch, ['from_zone', 'to_zone'], 'zone')\n",
    "    branch = branch.drop(columns=['from_interconnect', 'to_interconnect'])\n",
    "\n",
    "    print(\"getting util\")\n",
    "    # UTILIZATION DATA\n",
    "    util_median = get_utilization(grid.branch, pf, median=True).T\n",
    "    util_median = util_median.rename(columns={0: 'median_utilization'})\n",
    "\n",
    "    print(\"getting congestion stats\")\n",
    "\n",
    "    # RISK DATA (congestion)\n",
    "    congestion_stats = generate_cong_stats(pf, grid.branch)[['risk', 'bind']]\n",
    "\n",
    "    # Combine\n",
    "    util_and_cong = pd.concat([util_median, congestion_stats], axis=1)\n",
    "\n",
    "    # Replace any NANs with 0 for binding and risk\n",
    "    util_and_cong = util_and_cong.fillna(0)\n",
    "\n",
    "    # Set index as col so we keep the branch id when writing records to dict\n",
    "    util_and_cong = util_and_cong.reset_index()\n",
    "    branch_with_util_and_cong = branch.join(util_and_cong, on='branch_id')\n",
    "    branch_with_util_and_cong = branch_with_util_and_cong.reset_index()\n",
    "    \n",
    "    print()\n",
    "    print(branch_with_util_and_cong)\n",
    "    \n",
    "    return branch_with_util_and_cong\n",
    "\n",
    "# TODO: do we need other data?\n",
    "def get_grid_dcline(grid):\n",
    "    dcline = grid.dcline[['Pmax', \"from_bus_id\", \"to_bus_id\"]]\n",
    "    dcline = dcline.rename(columns={ 'Pmax': 'capacity' })\n",
    "\n",
    "    dcline = bus2coords(dcline, grid, bus_col_name='from_bus_id', col_prefix=\"from_\")\n",
    "    dcline = bus2coords(dcline, grid, bus_col_name='to_bus_id', col_prefix=\"to_\")\n",
    "    dcline = combine_cols(dcline, ['from_coords', 'to_coords'], 'coords')\n",
    "    dcline = combine_cols(dcline, ['from_interconnect', 'to_interconnect'], 'interconnect')\n",
    "    dcline = dcline.drop(columns=['from_zone', 'to_zone'])\n",
    "    \n",
    "    return dcline\n",
    "\n",
    "def get_grid_lmp(grid, lmp):\n",
    "    lmp_mean = lmp.mean()\n",
    "    lmp_mean = lmp_mean.to_frame()\n",
    "    lmp_mean.columns = ['lmp_mean']\n",
    "    \n",
    "    lmp_and_voltage = lmp_mean.join(grid.bus[['baseKV']])\n",
    "    lmp_and_voltage = lmp_and_voltage.rename(columns={'baseKV': 'voltage_level'})\n",
    "\n",
    "    # copy the index (bus_id) to a column so we can call bus2coords\n",
    "    lmp_and_voltage['bus_id'] = lmp_and_voltage.index\n",
    "    lmp_with_coords = bus2coords(lmp_and_voltage, grid, drop_bus_col=False)\n",
    "    \n",
    "    return lmp_with_coords\n",
    "    \n",
    "def get_plant_records(s, grid):\n",
    "    pg = s.state.get_pg()\n",
    "    wind = s.state.get_wind()\n",
    "    solar = s.state.get_solar()\n",
    "\n",
    "    plant_records = get_grid_plant(grid, pg, wind, solar).to_dict('records')\n",
    "    for plant in plant_records:\n",
    "        plant[\"capacity\"] = round(plant[\"capacity\"], 2)\n",
    "        plant[\"generation\"] = round(plant[\"generation\"], 2)\n",
    "        plant[\"curtailment\"] = round(plant[\"curtailment\"], 2)\n",
    "        \n",
    "    return plant_records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to records creates trailing numbers due to floating \n",
    "# point arithmetic so we round after converting to records \n",
    "# Other note: our local saved files have the .json.gzip extension, \n",
    "# but the blob storage files are just .json. The reason for this is\n",
    "# because it's important to know the local files are gzipped.\n",
    "# Deck.gl, however, only works with the .json extension and lets the\n",
    "# browser handle un-zipping the files\n",
    "def create_and_upload_grid_files_for_scenario(\n",
    "        scenario_id,\n",
    "        data_types,\n",
    "        blob_client, \n",
    "        path, \n",
    "        version):\n",
    "    print(f\"Creating grid files for {scenario_id}\")\n",
    "\n",
    "    s = Scenario(scenario_id)\n",
    "    grid = s.get_grid()\n",
    "    \n",
    "    blob_path=f\"{version}/{scenario_id}\"\n",
    "    \n",
    "    # Create local path if it doesn't exist\n",
    "    local_save_path=f\"{path}/{version}/{scenario_id}\"\n",
    "    Path(local_save_path).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    if (\"plant\" in data_types):\n",
    "        print(\"\\nCreating plant data...\")\n",
    "        plant_records = get_plant_records(s, grid)\n",
    "\n",
    "        print(\"Uploading plant data\")\n",
    "        blob_client.upload_dict_as_json_gzip(\n",
    "            plant_records, \n",
    "            f\"{local_save_path}/plant.json.gzip\",\n",
    "            f\"{blob_path}/plant.json\");\n",
    "        \n",
    "    if (\"emissions\" in data_types):\n",
    "        if (\"plant\" in data_types):\n",
    "            print(\"\\nUsing existing plant data for emissions...\")\n",
    "            emissions_records = plant_records\n",
    "        else:\n",
    "            print(\"\\nCreating emissions data...\")\n",
    "            emissions_records = get_plant_records(s, grid)\n",
    "        \n",
    "        emissions_records = list(filter(\n",
    "            lambda plant: plant[\"resource_type\"] == \"coal\" or plant[\"resource_type\"] == \"ng\", \n",
    "            emissions_records\n",
    "        ))\n",
    "        \n",
    "        print(\"Uploading emissions data\")\n",
    "        blob_client.upload_dict_as_json_gzip(\n",
    "            emissions_records, \n",
    "            f\"{local_save_path}/emissions.json.gzip\",\n",
    "            f\"{blob_path}/emissions.json\");\n",
    "        \n",
    "    if (\"storage\" in data_types):\n",
    "        print(\"\\nCreating storage data...\")\n",
    "        storage_records = get_grid_storage(grid).to_dict('records')\n",
    "        for storage in storage_records:\n",
    "            storage[\"capacity\"] = round(storage[\"capacity\"], 2)\n",
    "        \n",
    "        print(\"Uploading storage data\")\n",
    "        blob_client.upload_dict_as_json_gzip(\n",
    "            storage_records, \n",
    "            f\"{local_save_path}/storage.json.gzip\",\n",
    "            f\"{blob_path}/storage.json\");\n",
    "    \n",
    "    if (\"branch\" in data_types):\n",
    "        print(\"\\nCreating branch data...\")\n",
    "        pf = s.state.get_pf()\n",
    "        branch_records = get_grid_branch(grid, pf).to_dict('records')\n",
    "        for branch in branch_records:\n",
    "            branch[\"capacity\"] = round(branch[\"capacity\"], 2)\n",
    "            branch[\"median_utilization\"] = round(branch[\"median_utilization\"], 3)\n",
    "            branch[\"risk\"] = round(branch[\"risk\"], 2)\n",
    "            branch[\"bind\"] = round(branch[\"bind\"], 2)\n",
    "\n",
    "        print(\"Uploading branch data\")\n",
    "        blob_client.upload_dict_as_json_gzip(\n",
    "            branch_records, \n",
    "            f\"{local_save_path}/branch.json.gzip\",\n",
    "            f\"{blob_path}/branch.json\");\n",
    "        \n",
    "    if (\"dcline\" in data_types):\n",
    "        print(\"\\nCreating dcline data...\")\n",
    "        dcline_records = get_grid_dcline(grid).to_dict('records')\n",
    "        print(\"Uploading dcline data\")\n",
    "        for dcline in dcline_records:\n",
    "            dcline[\"capacity\"] = round(dcline[\"capacity\"], 2)\n",
    "            \n",
    "        print(\"Uploading dcline data\")\n",
    "        blob_client.upload_dict_as_json_gzip(\n",
    "            dcline_records, \n",
    "            f\"{local_save_path}/dcline.json.gzip\",\n",
    "            f\"{blob_path}/dcline.json\");\n",
    "        \n",
    "    if (\"lmp\" in data_types):\n",
    "        print(\"\\nCreating lmp data...\")\n",
    "        lmp = s.state.get_lmp()\n",
    "        lmp_records = get_grid_lmp(grid, lmp).to_dict('records')\n",
    "        for lmp in lmp_records:\n",
    "            lmp[\"lmp_mean\"] = round(lmp[\"lmp_mean\"], 2)\n",
    "            lmp[\"voltage_level\"] = round(lmp[\"voltage_level\"], 2)\n",
    "            \n",
    "        print(\"Uploading lmp data\")\n",
    "        blob_client.upload_dict_as_json_gzip(\n",
    "            lmp_records, \n",
    "            f\"{local_save_path}/lmp.json.gzip\",\n",
    "            f\"{blob_path}/lmp.json\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run new grid code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scenario_ids = [\"824\", \"1270\"] # list of strings\n",
    "data_types = [\"plant\", \"emissions\", \"storage\", \"branch\", \"dcline\", \"lmp\"] # options: [\"plant\", \"emissions\", \"storage\", \"branch\", \"dcline\", \"lmp\"]\n",
    "version = \"v2\" # e.g. \"v1\"\n",
    "local_path = \"./grid-data\" # Don't include slash at the end\n",
    "conn_str = os.environ.get(\"BLOB_STORAGE_CONN_STR\")\n",
    "\n",
    "for scenario_id in scenario_ids:\n",
    "    create_and_upload_grid_files_for_scenario(\n",
    "        scenario_id,\n",
    "        data_types,\n",
    "        BlobUtil(conn_str, 'grid-data'), \n",
    "        local_path, \n",
    "        version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
